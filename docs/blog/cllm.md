From programming to providing legal and health advice, large language models (LLMs) are revolutionizing the landscape of human lives. 

<p align="center"><img src="clm_objective.png" alt="autoregressive" width="350"></p>
<p align="center">Figure 1: illustration of conventional AR decoding: one token is generated at a time.</p>
In the following section, we'll introduce consistency large language models (CLLMs), a new family models developed with our proposed techniques to reduce inference latency with Jacobi decoding.

## Background: Jacobi Decoding
Jacobi decoding is first introduced by [this paper](https://arxiv.org/abs/2305.10427). 

<p align="center"><img src="jacobi_objective.png" alt="autoregressive" width="350"></p>
<p align="center">Figure 2: illustration of Jacobi decoding: n-token sequence is fed into the LLM and iterates until convergence.</p>

#### Limitations of Vanilla Jacobi Decoding

## Consistency LLMs (CLLMs)

##  Experiments


## Final words
We invite you to refer to the [our paper](TODO) for more details! Please stay tuned for the code release!
